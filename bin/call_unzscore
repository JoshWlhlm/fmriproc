#!/usr/bin/env python
from lazyfmri import (
    utils,
    dataset,
)
import os
import sys
import json
import pickle
import getopt
import numpy as np
import pandas as pd
import nibabel as nb
opj = os.path.join
gb = f"{utils.color.GREEN}{utils.color.BOLD}"
end = utils.color.END

def fetch_filepath(func, skip_run_id=False):

    # get bids components
    comps = utils.split_bids_components(func)

    # compile output name
    final_output=""
    for id in ['sub','ses','task','run','space','hemi']:
        if id in list(comps.keys()):
            if id == "sub":
                final_output+= f"{id}-{comps[id]}"
            else:
                if id == "run":
                    if not skip_run_id:
                        final_output+=f"_{id}-{comps[id]}"
                else:
                    final_output+=f"_{id}-{comps[id]}"

    base_dir = f"sub-{comps['sub']}"
    if "ses" in list(comps.keys()):
        base_dir = opj(base_dir, f"ses-{comps['ses']}")

    return final_output,base_dir

def unzscore(raw,data):

    if isinstance(raw, (np.ndarray,pd.DataFrame)):
        unz = raw*data["std"].T+data["avg"].T
        return unz.values
    else:
        r1 = raw.get_fdata()
        r2 = data["std"].get_fdata()[...,np.newaxis]
        r3 = data["avg"].get_fdata()[...,np.newaxis]

        # insert TR in header
        hdr = raw.header.copy()
        hdr["pixdim"][4] = float(data["tr"])

        # unzscore
        unz = r1*r2+r3
        return nb.Nifti1Image(unz, affine=raw.affine, header=hdr)

def read_data(func):

    if func.endswith(".nii.gz"):
        return nb.load(func)
    else:
        obj = dataset.Dataset(
            func, 
            use_bids=True, 
            filter_strategy='raw', 
            standardization='raw', 
            verbose=False
        )

        # get raw data
        raw = obj.fetch_fmri(dtype='raw')

        return raw

def calculate_std(raw):

    # get standard deviation and mean
    if isinstance(raw, (np.ndarray,pd.DataFrame)):
        stdev = raw.values.std(axis=0, keepdims=True)
        avg = raw.values.mean(axis=0, keepdims=True)

        tmp_data = {
            'std': stdev,
            'avg': avg
        }        
    else:
        data = raw.get_fdata()
        stdev = nb.Nifti1Image(data.std(axis=-1), affine=raw.affine, header=raw.header)
        avg = nb.Nifti1Image(data.mean(axis=-1), affine=raw.affine, header=raw.header)

        # save in pickle
        tmp_data = {
            'std': stdev,
            'avg': avg,
            'tr': raw.header["pixdim"][4]
        }

    return tmp_data

def bids_search(func, start_search=[], skip_run_id=False):
    
    # split into bids components to get run ID
    comps = utils.split_bids_components(func)
    
    output_base = f"sub-{comps['sub']}"
    if "ses" in list(comps.keys()):
        output_base = opj(output_base, f"ses-{comps['ses']}")

        if len(start_search) > 0:
            if not any("ses" in i for i in start_search):
                start_search += [f"ses-{comps['ses']}_"]

    for el in ["run","hemi","task","acq","space"]:

        if el in list(comps.keys()):

            add_el = True
            # define cases where element should not be appended
            if len(start_search) > 0:
                
                # don't add element if it's already in 'start_search'
                if any(el in i for i in start_search):
                    add_el = False

            # don't add run if skip_run_id
            if el == "run":
                if skip_run_id:
                    add_el = False
                    
            if add_el:
                start_search += [f"{el}-{comps[el]}_"]

    return start_search, output_base

def fetch_match_data(search_for, search_dir):

    # get standard deviation and mean
    match = utils.get_file_from_substring(search_for, search_dir)
    if isinstance(match, list):
        raise ValueError(f"Found {len(match)} matches with filters {search_for}: {match}")
    
    # utils.verbose(f"found match: '{match}'", True)
    with open(match, 'rb') as in_file:
        data = pickle.load(in_file)
    
    return data

def write_pickle(data, fname):
    
    # utils.verbose(f"Writing {pkl_file}", True)
    f = open(fname, "wb")
    pickle.dump(data, f)
    f.close()

@utils.validate_cli_inputs(required_keys=["subject", "session", "task"])
def main(context):

    r"""
---------------------------------------------------------------------------------------------------
call_unzscore

Hook that allows you to undo the zscoring of pybest output; this should be called twice: First, 
with the '--pre' flag in order to calculate the standard deviation and average for each input file 
that is going to be found (based on the given filters) and processed by pybest. Then, with the '--
post' flag, but keeping the other inputs the same, we'll undo the zscoring by placing files with
exactly the same name as the pybest output in pybest/<subject>/<session>/unzscored. To utilize 
this as input for 'spinoza_fitprfs', use '--psc'. This will redirect the file searcher to the 
'unzscored'-folder, and convert the inputs to percent signal change following M. Aqil's strategy. 
See 'call_prf' docs for more information.

Mandatory (required input):
  -s|--sub        subject ID (e.g., "008")
  -n|--ses        session ID  (e.g., "1")
  -t|--task       task ID (e.g., "2R" or "pRF")

Optional (flags with defaults):
  -f|--fprep      path to fMRIPrep root folder (e.g., derivatives/fmriprep). Default is the 
                  environmental variable `DIR_DATA_DERIV/fmriprep`
  -o|--out        path to pybest output. Default is the environmental variable `DIR_DATA_DERIV/
                  pybest`
  -p|--space      space ID. Default is `fsnative`

Options (extra):
  -h|--help       print this help text  
  --lh            only process left hemisphere files (default is to loop through both)
  --rh            only process right hemisphere files (default is to loop through both)
  --pre           use this *before* pybest; calculates the standard deviation and average and 
                  stores it in <out>/<subject>/<ses>/tmp [default option]
  --post          use this *after* pybest; undo the z-scoring using the output from '--tmp'. 
                  Default is '--pre', so this flag tells the script to run the post-pybest
                  version

Example:
  # simple pre-pybest call
  call_unzscore -s 999 -n 1 --pre

  # simple post-pybest call
  call_unzscore -s 999 -n 1 --post
  
  # pre-pybest call with specified input/output folders
  call_unzscore \
      -s 999 \
      -n 1 \
      -f $DIR_DATA_DERIV/fmriprep \
      -o $DIR_DATA_DERIV/pybest \
      --pre

---------------------------------------------------------------------------------------------------
    """

    subject = context.get("subject")
    session = context.get("session")
    task = context.get("task")
    fprep_dir = context.get("fprep_dir")
    out_dir = context.get("out_dir")
    space = context.get("space")
    hemi_list = context.get("hemi_list")
    pre = context.get("pre")
    skip_run_id = context.get("skip_run_id")

    # initiate search parameters
    base_dir = f'sub-{subject}'
    search_for = [f'sub-{subject}']

    if session is not None:
        if session != "all":
            search_for += [f'ses-{session}_']

    if task is not None:
        search_for += [f'task-{task}']

    # check if inputs are niftis
    is_nifti = False
    extension = "npy"
    exclude = None
    if isinstance(space, str):
        if not "fs" in space:
            is_nifti = True
            extension = "nii.gz"
            search_hemi = []

            search_for += ["desc-preproc_bold.nii.gz"]
            if space != "func":
                search_for += [f'space-{space}']
            else:
                exclude = "space-"
        else:
            search_for += [f'space-{space}']
    
    # pre-pybest workflow; saves std and mean
    if pre:

        if is_nifti:

            # final func_dir
            ffunc_dir = utils.FindFiles(opj(fprep_dir, f'sub-{subject}'), extension="nii.gz").files

            # load data
            funcs = utils.get_file_from_substring(search_for, ffunc_dir, exclude="json")
            if isinstance(funcs, str):
                skip_run_id = True
                funcs = [funcs]

            # get session IDs
            ses_ids = utils.get_ids(funcs, bids="ses")
            if len(ses_ids) > 0:
                for ses in ses_ids:
                    
                    ses_funcs = utils.get_file_from_substring([f"ses-{ses}_"], funcs, exclude=exclude)
                    if isinstance(ses_funcs, str):
                        ses_funcs = [ses_funcs]
                    
                    # get run ids
                    skip_run_id = False
                    run_ids = utils.get_ids(ses_funcs, bids="run")
                    if len(run_ids) == 1:
                        skip_run_id = True

                    for func in ses_funcs:
                        
                        utils.verbose(f" {gb}{func}{end}", True)
                        # construct output basename and base output dir
                        final_output, base_dir = fetch_filepath(func, skip_run_id=skip_run_id)

                        # make output directory
                        output_dir = opj(out_dir, base_dir, 'tmp')
                        if not os.path.exists(output_dir):
                            os.makedirs(output_dir, exist_ok=True)
                        
                        # read in file
                        raw = read_data(func)
                        write_data = calculate_std(raw)
                        
                        # write file
                        pkl_file = opj(output_dir, final_output+'_desc-avgstd.pkl')
                        write_pickle(write_data, pkl_file)
            else:
                # get run ids
                skip_run_id = False
                run_ids = utils.get_ids(funcs, bids="run")
                if len(run_ids) == 1:
                    skip_run_id = True

                for func in funcs:
                    
                    utils.verbose(f" {gb}{func}{end}", True)
                    # construct output basename and base output dir
                    final_output, base_dir = fetch_filepath(func, skip_run_id=skip_run_id)

                    # make output directory
                    output_dir = opj(out_dir, base_dir, 'tmp')
                    if not os.path.exists(output_dir):
                        os.makedirs(output_dir, exist_ok=True)

                    # read in file
                    raw = read_data(func)
                    write_data = calculate_std(raw)   
                    
                    # write file
                    pkl_file = opj(output_dir, final_output+'_desc-avgstd.pkl')
                    write_pickle(write_data, pkl_file)  
        else:

            for hemi in hemi_list:

                if hemi is not None:
                    search_hemi = search_for + [f'hemi-{hemi}']    

                if pre:

                    # final func_dir
                    ffunc_dir = utils.FindFiles(opj(fprep_dir, f'sub-{subject}'), extension="gii").files

                    # load data
                    funcs = utils.get_file_from_substring(search_hemi, ffunc_dir, exclude="json")
                    if isinstance(funcs, str):
                        skip_run_id = True
                        funcs = [funcs]

                    # get session IDs
                    ses_ids = utils.get_ids(funcs, bids="ses")
                    if len(ses_ids) > 0:
                        for ses in ses_ids:
                            
                            ses_funcs = utils.get_file_from_substring([f"ses-{ses}_"], funcs)
                            if isinstance(ses_funcs, str):
                                ses_funcs = [ses_funcs]
                            
                            # get run ids
                            skip_run_id = False
                            run_ids = utils.get_ids(ses_funcs, bids="run")
                            if len(run_ids) == 1:
                                skip_run_id = True

                            for func in ses_funcs:
                                
                                utils.verbose(f" {gb}{func}{end}", True)
                                # construct output basename and base output dir
                                final_output, base_dir = fetch_filepath(func, skip_run_id=skip_run_id)

                                # make output directory
                                output_dir = opj(out_dir, base_dir, 'tmp')
                                if not os.path.exists(output_dir):
                                    os.makedirs(output_dir, exist_ok=True)
                                
                                # read in file
                                raw = read_data(func)
                                write_data = calculate_std(raw)

                                # write file
                                pkl_file = opj(output_dir, final_output+'_desc-avgstd.pkl')
                                write_pickle(write_data, pkl_file)
                    else:
                        # get run ids
                        skip_run_id = False
                        run_ids = utils.get_ids(funcs, bids="run")
                        if len(run_ids) == 1:
                            skip_run_id = True

                        for func in funcs:
                            
                            # print
                            utils.verbose(f" {gb}{func}{end}", True)

                            # construct output basename and base output dir
                            final_output, base_dir = fetch_filepath(func, skip_run_id=skip_run_id)

                            # make output directory
                            output_dir = opj(out_dir, base_dir, 'tmp')
                            if not os.path.exists(output_dir):
                                os.makedirs(output_dir, exist_ok=True)

                            # read in file
                            raw = read_data(func)
                            write_data = calculate_std(raw)

                            # write file
                            pkl_file = opj(output_dir, final_output+'_desc-avgstd.pkl')
                            write_pickle(write_data, pkl_file)                                       
    else:
        
        # find all files
        all_files = utils.FindFiles(opj(out_dir, base_dir), extension=extension).files

        # load data; check if we can load with 'run' flag, otherwise we only have a single run
        full_search = search_for + ['denoised_bold','denoising']

        # filter out remnants from nifti-specification
        if is_nifti:
            full_search = utils.get_file_from_substring([], full_search, exclude="desc-preproc_bold.nii.gz")

        funcs = utils.get_file_from_substring(full_search, all_files, exclude="raw")

        if isinstance(funcs, str):
            funcs = [funcs]

        if len(funcs) == 0:
            raise ValueError(f"No files with {full_search} in '{opj(out_dir, base_dir)}/*'")

        # get session IDs
        ses_ids = utils.get_ids(funcs, bids="ses")
        if len(ses_ids) > 0:
            for ses in ses_ids:

                ses_funcs = utils.get_file_from_substring([f"ses-{ses}/", f"ses-{ses}_"], funcs, exclude=exclude)
                if isinstance(ses_funcs, str):
                    ses_funcs = [ses_funcs]
                
                # get run ids
                skip_run_id = False
                run_ids = utils.get_ids(ses_funcs, bids="run")
                if len(run_ids)<2:
                    skip_run_id = True  

                # filter out run-ID if present
                if not skip_run_id:
                    ses_funcs = utils.get_file_from_substring(["run-"], ses_funcs)

                # make string again
                if isinstance(ses_funcs, str):
                    ses_funcs = [ses_funcs]
                
                for func in ses_funcs:
                    
                    # split into bids components to get run ID
                    all_search, output_base = bids_search(func, start_search=[], skip_run_id=skip_run_id)

                    # set tmp directory containing pkl-files
                    avg_dir = opj(out_dir, output_base, 'tmp')
                    output_dir = opj(out_dir, output_base, 'unzscored')

                    if not os.path.exists(output_dir):
                        os.makedirs(output_dir, exist_ok=True)

                    # read in file; returns nibabel.Nifti1Image if extension == "nii.gz"
                    raw = read_data(func)

                    # find matching pkl-file
                    data = fetch_match_data(all_search, avg_dir)
                    
                    # undo zscore
                    unzscored = unzscore(raw,data)

                    # sort nifti dimensions
                    fname = opj(output_dir, os.path.basename(func).replace("denoised", "unzscored"))
                    utils.verbose(f" Writing {gb}{fname}{end}", True)
                    if extension == "nii.gz":
                        unzscored.to_filename(fname)
                    else:
                        np.save(fname, unzscored.T)

                    # check json file
                    json_file = func.replace(extension, "json")
                    if os.path.exists(json_file):
                        with open(json_file, "r+") as f:
                            data = json.load(f)

                        data["Unzscored"] = True
                        data["SkullStripped"] = False
                        new_json = fname.replace(extension, "json")
                        with open(new_json, "w") as f:
                            json.dump(data, f, indent=4)
        else:   

            # get run ids
            skip_run_id = False
            run_ids = utils.get_ids(funcs, bids="run")
            if len(run_ids) == 1:
                skip_run_id = True

            if not skip_run_id:
                funcs = utils.get_file_from_substring(["run-"], funcs)
            
            for func in funcs:

                # split into bids components to get run ID
                all_search, output_base = bids_search(func, start_search=[], skip_run_id=skip_run_id)

                # set tmp directory containing pkl-files
                avg_dir = opj(out_dir, output_base, 'tmp')
                output_dir = opj(out_dir, output_base, 'unzscored')

                if not os.path.exists(output_dir):
                    os.makedirs(output_dir, exist_ok=True)

                # read in file
                raw = read_data(func)

                # find matching pkl-file
                data = fetch_match_data(all_search, avg_dir)

                # undo zscore
                unzscored = unzscore(raw,data)

                # sort nifti dimensions
                fname = opj(output_dir, os.path.basename(func).replace("denoised", "unzscored"))
                utils.verbose(f" Writing {gb}{fname}{end}", True)
                if extension == "nii.gz":
                    unzscored.to_filename(fname)
                else:
                    np.save(fname, unzscored.T)

                # check json file
                json_file = func.replace(extension, "json")
                if os.path.exists(json_file):
                    with open(json_file, "r+") as f:
                        data = json.load(f)

                    data["Unzscored"] = True
                    data["SkullStripped"] = False
                    new_json = fname.replace(extension, "json")
                    with open(new_json, "w") as f:
                        json.dump(data, f, indent=4)

if __name__ == "__main__":

    # set defaults
    subject     = None
    session     = None
    task        = None
    fprep_dir   = opj(os.environ.get("DIR_DATA_DERIV"), 'fmriprep')
    out_dir     = opj(os.environ.get("DIR_DATA_DERIV"), 'pybest')
    space       = "fsnative"
    hemi_list   = ['L', 'R']
    pre         = True
    skip_run_id = False

    try:
        opts = getopt.getopt(
            sys.argv[1:],
            "hf:o:s:n:t:h:p:",
            ["fprep=", "lh", "rh", "out=", "sub=", "ses=", "task=", "space=", "hemi=", "post", "pre", "help"]
        )[0]
    except getopt.GetoptError:
        utils.verbose(main.__doc__, True)
        sys.exit(2)

    for opt, arg in opts:
        if opt in ('-h', 'help'):
            utils.verbose(main.__doc__, True)
            sys.exit()
        elif opt in ("-f", "--fprep"):
            fprep_dir = arg
        elif opt in ("-o", "--out"):
            out_dir = arg
        elif opt in ("-s", "--sub"):
            subject = arg
        elif opt in ("-n", "--ses"):
            session = arg
        elif opt in ("-t", "--task"):
            task = arg
        elif opt in ("-p", "--space"):
            space = arg
        elif opt in ("--lh"):
            hemi_list = ["L"]
        elif opt in ("--rh"):
            hemi_list = ["R"]
        elif opt in ('--pre'):
            pre = True
        elif opt in ('--post'):
            pre = False

    main(context={
        "subject": subject,
        "session": session,
        "task": task,
        "fprep_dir": fprep_dir,
        "out_dir": out_dir,
        "space": space,
        "hemi_list": hemi_list,
        "pre": pre,
        "skip_run_id": skip_run_id
    })
